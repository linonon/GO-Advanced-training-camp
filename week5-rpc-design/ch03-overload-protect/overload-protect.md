# Overload Protect

令牌痛算法，按照固定速率往桶裡添加令牌。

- 假設限制 2r/s，則按照500ms 的固定速率往桶中添加令牌。
- 桶中最多存放b個令牌，當桶滿時，新添加的令牌被丟棄或拒絕。
- 當一個 n 個字節大小的數據包到達，將從桶中刪除 n個令牌，接著數據包被發送到網路上。
- 如果桶中的令牌不足 n 個， 則不會刪除令牌，且該數據包將被限流（要麼丟棄，要麼緩衝區等待）

![](/week5-rpc-design/pic/令牌桶.png)
<center>令牌桶</center>

漏桶算法：流量整形

- 一個固定容量的漏桶，按照常量固定速率流出水滴
- 如果桶空，則不需要流出水滴
- 可以以任意速率流入水滴到漏桶
- 如果流入超過容量，則溢出的被丟棄

![](/week5-rpc-design/pic/漏桶.png)
<center>漏桶</center>

上面兩個方法思路都是當`超過裡指標後就阻止`或減少流量的繼續進入，然後當系統負載`降低到某一水平後則回覆`流量的進入。但這種通常都是被動的，其實際效果取決於`限流閥的值設置是否合理`，但找到合理的值是很難的一件事。

- 集群增加機器或者減少機器時，`限流閥值`是否要重新設置？
- 值的設置依據又是什麼？
- 人力運帷成本是否過高？
- 當調用方反饋429時，這個時候重新設置限流，其實流量高峰已經過了重新評估限流是否有意義？

- 系統保護
  - 當服務器鄰近過載時，主動拋棄一定量的負載，目標是自保。
  - 在系統穩定的前提下，保持系統的吞吐量。（Little's law）
  - CPU、內存作為信號量節流
  - 隊列管理：隊列長度、LIFO
  - 可控延遲算法：CoDel

- 如何計算接近峰值時的系統吞吐？
  - CPU：使用一個獨立的線程採樣，每格250ms觸發一次，在計算均值時，使用了簡單滑動平均去除峰值的影響。
  - Inflight：當前服務中正在進行的請求數量
  - Pass&RT：最近5s，pass為每100ms 採樣窗口成功請求的數量，rt為單個採樣窗口中平均響應時間。

- 過載保護舉例：
  - 限流效果生效後，CPU會在臨界值附近抖動，如果不使用冷卻時間，那麼一個段時間的CPU下降就可能導致大量請求被放行，嚴重時會打滿CPU
  - 冷卻時間後，重新判斷閾值(CPU > 800)，是否持續進入過載保護。

# 限流

限流是指，在一段時間內定義某個客戶或者應用可以接受或處理多個請求的技術。例如，通過限流，你可以過濾掉產生流量峰值的客戶和微服務，或者可以確保你的應用在`自動擴展（Auto Scaling）`失效錢都不會出現過載的情況。

- 令牌桶、漏桶針對單個節點，無法`分布式限流`
- QPS限流
  - 不同的請求可能需要數量迥異的資源處理
  - 某種靜態QPS限流不是特別準
- 給每個用戶設置限制
  - 全局過載發生時候，針對某些“異常控制”進行控制
  - 一定程度的“超賣”配額
- 按照優先級丟棄
- 拒絕請求也需要成本（`返回429`之類的）

## 分布式限流

- v1: 經典：Redis 緩存限流
  - 單個大流量接口，容易產生熱點
  - `pre-request` 模式對性能有一定影響，高頻網絡的往返。
- v2: 異步批量獲取 quota，可以大大減少 redis 請求，獲取完以後本地消費，基於令牌桶攔截。
  - 每次配額需要手動配置
- v3: Max-Min Fairness
  - 直觀上，公平分享分配給每個用戶要的可以滿足的最小需求，然後將沒有使用的資源均勻分配給需要的大資源用戶。
  - 資源按需求遞增的順序進行分配。
  - 不存在用戶得到的資源超過自己的需求。
  - 未得到滿足的用戶等價的分享資源。

![](/week5-rpc-design/pic/Max-Min-Fairness.png)
<center>Max-Min-Fairness</center>

不同類型限流的對比
![](/week5-rpc-design/pic/不同限流.png)

## 重要性

- CRITICAL_PLUS：拒絕這些請求會有非常嚴重的用戶可見的問題
- CRITICAL：拒絕後，用戶也會有問題，不過沒那麼嚴重
- SHEDDABLE_PLUS：可以容忍某種程度的不可用性，這些請求通常可以過幾分鐘～幾小時後重試。
- SHEDDABLE：這些流量可能會經常遇到部分不可用情況，偶爾會完全不可用。

配額順序：

- 優先拒絕低優先的請求
- 全局配額，可以按照重要性分別設置
- 過載保護時，低優先級請求會被丟棄

## 熔斷

為了限制操作的持續時間，可以使用超時，超時可以防止掛起操作並保證系統可以響應。

- 服務依賴的資源出現大量錯誤
- Server端有可能因為不斷發送拒絕請求而導致過載。

- `Google SRE`: `max(0, (requests - K * accepts) / (requests + 1))`
  - 隨著 requests 增加， accepts 也會增加。
  
```go
func (b *sreBreaker) Allow() error {
    success, total := b.stat.Value()
    k := b.k * float64(success)

    if total < b.request || float64(total) < k {
        return nil
    }
    dr := math.Max(0, (float64(total) - k)/float64(total + 1))
    rr := b.r.Float64()

    if dr <= rr {
        return nil
    }
    return ecode.ServiceUnavailable
}
```

## Gutter

基於熔斷的gutter kafka， 用於接管自動修復系統運行過程中的負載，這樣只需要付出10%的資源就能解決部分系統可用性問題。

## 客戶端控流

positive feedback：用戶總是積極重試，訪問一個不可控的服務。

- 客戶端限制請求頻次，retry backoff做一定的請求退讓。
- 可以通過接口級別的error_details，掛載到每個API返回到響應裡。
